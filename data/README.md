## Results From the Paper

The paper discusses 3 experiments.
Each of these experiments is an ensemble of 32 individual models.

Each of the 3 experiments differed in the manner in which weights were assigned to each of the 3 branches in the model architecture as they were merged.

In the sub-directories of this directory in this repository you will find summary-level information about each of these 3 experiments.
The training events and saved weights of the trainable parameters for all 96 models will be supplied upon requests made to abyerly@fsmail.bradley.edu, but are not included in this repository due to their size (23GB).

1. The 32 models that were trained in the first experiment were merged with equal weight being given to each of the 3 branches.  The results of this experiment can be found in the ``not_learnable`` sub-directory of this directory.

2. The 32 models that were trained in the second experiment were merged with weights that were learned during back-propagation.  The (3) weights were intialized from a glorot uniform random distribution.  The results of this experiment can be found in the ``learnable_rand_init`` sub-directory of this directory.

3. The 32 models that were trained in the second experiment were merged with weights that were learned during back-propagation.  The (3) weights were all intialized to 1.  The results of this experiment can be found in the ``learnable_ones_init`` sub-directory of this directory.

The following information can be found in each of the experiments' sub-directories.

### all_wrong
This directory contains renderings of the MNIST evaluation digits that no model in the experiment predicted correctly.

Note: _Files are named with the following  pattern:_ ``XXXX(Y).gif`` _, where_ ``XXXX`` _is the position of the image in the MNIST evluation data and_ ``Y`` _is the label for that image._

### disagreeing
This directory contains renderings of the MNIST evaluation digits that were predicted correctly by at least one model in the experiment and incorrectly by at least one model.

_See the note in the_ **all_wrong** _section above regarding the interpretation of the files' names in this directory._

### ensemble_data.txt
This is the file created by executing ``python/etc/ensemble_evaluations.py`` from this repository.
This file is used by the C++/CUDA code in this repository. (_See [here](../C%2B%2B#evaluating-ensemble-model-combinations) for more information regarding the correct interpretation of this file's contents._)

### ensemble_info.xlsx
This file was created manually from the CSVs created by executing ``python/etc/extract_scalars_from_logs.py``, ``ensemble_data.txt``, and from the standard output generated by the binary compiled from the C++/CUDA code in this repository (_See [here](../C%2B%2B#evaluating-ensemble-model-combinations) for more information._).

This workbook includes 4 sheets:
- **Ensembles** - (From the standard output generated by C++/CUDA binary.)  This sheet lists the ensembles found by the C++/CUDA binary and the accuracy they achieved.  Only accuracies of 9982/10000 or higher are reported.
- **Disagreements** - (From ``ensemble_data.txt``.)  This sheet lists the correct labels of and all of the predictions for any predictions that were made correctly by at least one model in the experiment and incorrectly by at least one model.
- **Top1s** - (From the CSVs created by ``python/etc/extract_scalars_from_logs.py``.) This sheet lists the top-1 prediction accuracies achieved after each epoch of training for each trial in the experiment.  In addition, the best top-1 for each trial is in the second column and below the one row for each trial is the average and standard deviation for all trials in the experiment.
- **Losses** - (From the CSVs created by ``python/etc/extract_scalars_from_logs.py``.) This sheet lists the losses achieved after each epoch of training for each trial in the experiment.  In addition, the best losses for each trial is in the second column and below the one row for each trial is the average and standard deviation for all trials in the experiment.
